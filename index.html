<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Valerie Chen</title>
  
  <meta name="author" content="Valerie Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101241284);</script>
  <script async src="//static.getclicky.com/js"></script>
  
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

    <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <p style="text-align:center">
                <name>Valerie Chen</name>
              </p>

          <img style="width:25%;max-width:25%" alt="profile photo" src="images/headshot.png" class="center">

          <tr>
            <td>

        <p> I'm a third year <a href="https://www.ml.cmu.edu/"> Machine Learning</a> PhD student at Carnegie Mellon University advised by <a href="https://www.cs.cmu.edu/~atalwalk/"> Ameet Talwalkar</a> and my PhD is supported by the NSF Graduate Research Fellowship. I previously interned at Microsoft Research in the <a href="https://www.microsoft.com/en-us/research/theme/fate/">FATE (Fairness, Accountability, Transparency, and Ethics of AI)</a> group with <a href="http://www.qveraliao.com/">Q. Vera Liao</a> and <a href="http://www.jennwv.com/">Jennifer Wortman Vaughan</a>. I completed my BS in Computer Science at Yale University, where I previously worked with <a href="https://www.cs.yale.edu/homes/shao-zhong/"> Zhong Shao</a> and <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>. I have also spent time at IBM Research and the Naval Research Laboratory. 

        <br><br> I am broadly interested building tools and theory to improve <b>human-ML interactions</b>. I believe it is important to design ML systems with downstream users in mind: What expertise does the user bring to the human-ML team? How will the user utilize explanations about model behavior? I am also thinking about how to make human-centered study of these interactions more efficient and comprehensive.

        <!-- Some questions that I have been thinking about recently to change this:
        <ul>
          <li>ML models learn to minimize loss. How do you make use of human feedback and expertise to learn more complementary models?</li>
          <li>Explanations are often designed only to be faithful to the model, but ultimately are used by humans. What are ways to design explanations of ML models for real-world use case?</li>
          <li>Human-ML systems are ideally studied with user studies, but existing approaches often preclude the consideration of many design decisions. How do you make this process more efficient? What aspects can be automated?</li>
        </ul> -->


        <!-- <br><br> I also occasionally tweet at <a href="https://twitter.com/valeriechen_"> @valeriechen_</a>. -->


          </p>

              <p style="text-align:center">
                <a href="mailto:valeriechen@cmu.edu">Email</a> /
                <a href="https://scholar.google.com/citations?user=94yn2j0AAAAJ&hl=en">Google Scholar</a> / <a href="https://twitter.com/valeriechen_"> Twitter </a>
                <!-- <a href="https://www.linkedin.com/in/valeriechen11/"> LinkedIn </a> /
                <a href="https://github.com/valeriechen"> Github </a> -->
              </p>
          </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Conference/Journal Papers</heading>
              <br><br>


              <papertitle>Use-Case-Grounded Simulations for Explanation Evaluation</papertitle>
              <br>
              <strong>Valerie Chen</strong>,
              Nari Johnson, Nicholay Topin*, Gregory Plumb*, Ameet Talwalkar
              <br>
              <em>NeurIPS, 2022</em>
              <!-- <em>NeurIPS XAI4Debugging Workshop, 2021</em> -->
              <br>
              <a href="https://arxiv.org/abs/2206.02256">[Link]</a>
              <br><br>

              
              <papertitle>Bayesian Persuasion for Algorithmic Recourse</papertitle>
              </a>
              <br>
              Keegan Harris,
              <strong>Valerie Chen</strong>,
              Joon Sik Kim, Ameet Talwalkar, Hoda Heidari, Steven Wu
              <br>
              <em>NeurIPS, 2022</em>
              <!-- <em>NeurIPS Workshop on Human and Machine Decisions</em>, 2021 (Oral)  -->
              <!-- <br>
              <em>3nd Symposium on the Foundations of Responsible Computing, 2022</em> <br>
              <em>AAMAS Learning with Strategic Agents Workshop, 2022 (Oral)</em> -->
              <br>               <a href="https://arxiv.org/abs/2112.06283">[Link]</a>

              <br><br>


              <papertitle>Interpretable Machine Learning: Moving From Mythos to Diagnostics</papertitle>
              <br>
              <strong>Valerie Chen*</strong>,
              Jeffrey Li*,
              Joon Sik Kim**,
              Gregory Plumb**,
              Ameet Talwalkar
              <br>
              <!-- <em>ICML HILL Workshop, 2021</em>
              <br>
              <em>ACM Queue, 2022 </em>
              <br> -->
              <em>Communications of ACM, 2022</em>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3511299">[Link]</a>
              <br><br>

              
              <papertitle>Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Valerie Chen</strong>,
              Abhinav Gupta,
              Kenneth Marino
              <br>
              <!-- <em>Robotics Institute Summer Scholar Poster Presentation, 2019</em>
              <br> -->
              <em>ICLR, 2021</em>
              <br>               <a href="https://arxiv.org/abs/2011.00517">[Link]</a>

              <br><br>
              <papertitle>Task-Aware Novelty Detection for Visual-based Deep Learning in Autonomous Systems</papertitle>
              <br>
              <strong>Valerie Chen</strong>,
              Man-Ki Yoon,
              Zhong Shao
              <br>
              <em>ICRA, 2020</em>
              <br>
              <a href="https://flint.cs.yale.edu/flint/publications/novelty.pdf">[Link]</a>


            </td>
          </tr>
        </tbody></table>


       
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Workshop Papers</heading>
              <br><br>


              <papertitle>Perspectives on Incorporating Expert Feedback into Model Updates</papertitle>
              <br>
              <strong>Valerie Chen*</strong>,
              Umang Bhatt*, Hoda Heidari, Adrian Weller, Ameet Talwalkar
              <br>
              <em>ICML Workshop on Human-Machine Collaboration and Teaming, 2022</em>
              <!-- <br>
              <em>ICML Workshop on Responsible Decision Making in Dynamic Environments, 2022 </em>
              <br>
              <em>ICML Workshop on Updatable Machine Learning, 2022</em> -->
              <br>
              <!-- <em>Under Review </em> -->
              <!-- <br> -->
              <a href="https://arxiv.org/abs/2205.06905">[Link]</a>
              <br><br>


              <papertitle>Can Explanations Help Users with Text Comprehension?</papertitle>
              <br>
              Joon Sik Kim, <strong>Valerie Chen</strong>,
              Nihar Shah, Ameet Talwalkar
              <br>
              <em>NAACL 2nd HCI+NLP Workshop, 2022</em>
              <br><br>


              
              <papertitle>Explainable Deep Learning for Visual-based Safety-critical Systems</papertitle>
              <br>
              <strong>Valerie Chen</strong>,
              Man-Ki Yoon,
              Zhong Shao
              <br>
              <em>DSN DSML Workshop</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1906.03685">[Link]</a>
              <br><br>

              <papertitle>Video-Text Compliance: Activity Verification based on Natural Language Instructions</papertitle>
              <br>
              Mayoore Jaiswal, Frank Liu, Anupama Jagannathan, Anne Gattiker, Inseok Hwang, Jinho Lee, Matt Tong, Sahil Dureja, Soham Shah, Peter Hofstee, 
              <strong>Valerie Chen</strong>, Suvadip Paul, Rogerio Feris
              <br>
              <em>Spotlight Paper at ICCV Closing the Loop Between Vision and Language Workshop, 2019 <br> Oral Presentation at ICCV Large Scale Holistic Video Understanding Workshop</em>, 2019

              <br><br>
                <papertitle>Secure Computation for Machine Learning With SPDZ</papertitle>
              <br>
              <strong>Valerie Chen</strong>,
              Valerio Pastro,
              Mariana Raykova
              <br>
              <em>NeurIPS PPML Workshop, 2018 <br> 1st Place at ACM Student Research Competition at Grace Hopper Conference, 2018</em>
              <br>               <a href="https://arxiv.org/abs/1901.00329">[Link]</a>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Preprints</heading>
              <br><br>

              <papertitle>Best Practices for Interpretable Machine Learning in Computational Biology</papertitle>
              <br>
              <strong>Valerie Chen*</strong>,
              Muyu Yang*, Wenbo Cui, Joon Sik Kim, Ameet Talwalkar, Jian Ma
              <br>
              <a href="https://www.biorxiv.org/content/10.1101/2022.10.28.513978v1">[Link]</a>
              <br><br>

              <papertitle>On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods</papertitle>
              <br>
              Kasun Amarasinghe, Kit Rodolfa, Sergio Jesus, <strong>Valerie Chen</strong>, Vladimir Balayan, Pedro Saleiro, Pedro Bizarro, Ameet Talwalkar, Rayid Ghani
              <br>
              <a href="https://arxiv.org/abs/2206.13503">[Link]</a>

            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
              <br><br>
              07-300 Research and Innovation TA, Fall 2021
              <br><br>
              Computer Science Department Peer Mentor, 2019-2020
              <br><br>
              CPSC 470 Artificial Intelligence, Spring 2019 and 2020
              <br><br>
              CPSC 201 Intro CS, Fall 2017
              <br>
            </td>
          </tr>
        </tbody></table>
        

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Important non-research things</heading>
              <br><br>
              DEI Course at CMU (<a href="https://www.cs.cmu.edu/~15996/"> check out the course website </a>) 
              <br><br>
              Yale Women's Leadership Initiative (<a href="https://www.yalewli.org/">support remembering 50 at Yale here </a>)
              <br><br>
              SheCode at Yale (<a href="https://shecodeatyale.wordpress.com/"> org website</a>)
              <br><br>
              Computation and Society at Yale (<a href="https://computationsociety.yale.edu/"> yalies should get involved here </a>)
              <br>
            </td>
          </tr>
        </tbody></table>

        <br>
        Last updated: 11/1/2022

      </td>
    </tr>


  </table>


</body>

</html>
